{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://bhavikjikadara.medium.com/data-science-automation-a-step-by-step-guide-using-crewai-e1468823e0f8\n",
    "\n",
    "\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from crewai.tools import tool\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"groq/llama-3.3-70b-versatile\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "# llm = LLM(\n",
    "#     model=\"ollama/mistral\",\n",
    "#     base_url=\"http://localhost:11434\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from crewai_tools import BaseTool\n",
    "\n",
    "class DataPreprocessor(BaseTool):\n",
    "    name: str = \"Data Preprocessor\"\n",
    "    description: str = \"Preprocesses data by handling missing values, removing duplicates, and encoding categorical variables.\"\n",
    "\n",
    "    def _run(self, file_path: str) -> str:\n",
    "        # Load the data\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Get initial info\n",
    "        initial_shape = df.shape\n",
    "        initial_missing = df.isnull().sum().sum()\n",
    "        \n",
    "        # Calculate the percentage of missing values\n",
    "        missing_percentage = (initial_missing / df.size) * 100\n",
    "        \n",
    "        # Handle missing values\n",
    "        if missing_percentage < 5:\n",
    "            df = df.dropna()\n",
    "        else:\n",
    "            # Use SimpleImputer for numerical columns\n",
    "            num_cols = df.select_dtypes(include=['number']).columns\n",
    "            num_imputer = SimpleImputer(strategy='mean')\n",
    "            df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "            \n",
    "            # Use SimpleImputer for categorical columns\n",
    "            cat_cols = df.select_dtypes(include=['object']).columns\n",
    "            cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "        \n",
    "        # Remove duplicate entries\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        # Identify categorical columns\n",
    "        categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        # Convert categorical variables to numerical\n",
    "        label_encoder = LabelEncoder()\n",
    "        for col in categorical_columns:\n",
    "            df[col] = label_encoder.fit_transform(df[col])\n",
    "        \n",
    "        # Get final info\n",
    "        final_shape = df.shape\n",
    "        final_missing = df.isnull().sum().sum()\n",
    "        \n",
    "        # Save the processed data\n",
    "        processed_file_path = os.path.join('processed_data', 'processed_data.csv')\n",
    "        os.makedirs(os.path.dirname(processed_file_path), exist_ok=True)\n",
    "        df.to_csv(processed_file_path, index=False)\n",
    "        \n",
    "        return f\"\"\"\n",
    "        Data preprocessing completed:\n",
    "        - Initial shape: {initial_shape}\n",
    "        - Initial missing values: {initial_missing}\n",
    "        - Final shape: {final_shape}\n",
    "        - Final missing values: {final_missing}\n",
    "        - Categorical variables encoded: {categorical_columns}\n",
    "        - Duplicates removed\n",
    "        - Processed data saved to: {processed_file_path}\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import DirectoryReadTool\n",
    "\n",
    "docs_tool_a = DirectoryReadTool(directory='data.csv')\n",
    "data_processing_tool = DataPreprocessor()\n",
    "data_preprocessing_agent = Agent(\n",
    "    role=\"Data Preprocessing Specialist\",\n",
    "    goal=\"Load, clean, and perform initial transformations on datasets\",\n",
    "    backstory=\"Expert in data cleaning and preprocessing using pandas, numpy, and sklearn libraries\",\n",
    "    llm=llm,\n",
    "    tools=[docs_tool_a, data_processing_tool],\n",
    "    verbose=True\n",
    "    # allow_code_execution=True\n",
    ")\n",
    "\n",
    "data_preprocessing_task = Task(\n",
    "  description=\"\"\"\n",
    "  Load the file, handle missing values (remove missing values if number of missing values is less than 5 percent of dataset else use imputer), remove duplicates, and convert categorical variables to numerical values to make the dataset model-ready.\n",
    "  \"\"\",\n",
    "  expected_output='Processed dataset saved successfully',\n",
    "  agent=data_preprocessing_agent,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = Crew(\n",
    "    agents=[data_preprocessing_agent], \n",
    "    tasks=[data_preprocessing_task], \n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "result = crew.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import tool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from crewai_tools import BaseTool\n",
    "\n",
    "# llm = LLM(\n",
    "#     model=\"ollama/mistral\",\n",
    "#     base_url=\"http://localhost:11434\"\n",
    "# )\n",
    "\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"groq/llama-3.3-70b-versatile\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class FeatureEngineeringTool(BaseTool):\n",
    "    name: str = \"Feature Scaling Tool\"\n",
    "    description: str = \"Scales numerical features to a standard range using techniques like normalization or standardization also encode categorical values\"\n",
    "\n",
    "    def _run(self, file_path: str) -> str:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df_engineered = df.copy()\n",
    "            \n",
    "            # Encode categorical variables\n",
    "            le = LabelEncoder()\n",
    "            categorical_cols = df_engineered.select_dtypes(include=['object']).columns\n",
    "            for col in categorical_cols:\n",
    "                df_engineered[col] = le.fit_transform(df_engineered[col].astype(str))\n",
    "            \n",
    "            # Scale numerical features\n",
    "            scaler = StandardScaler()\n",
    "            numerical_cols = df_engineered.select_dtypes(include=['int64', 'float64']).columns\n",
    "            df_engineered[numerical_cols] = scaler.fit_transform(df_engineered[numerical_cols])\n",
    "\n",
    "            output_path = 'engineered_features.csv'\n",
    "            df_engineered.to_csv(output_path, index=False)\n",
    "            \n",
    "            return f\"file saved to {output_path}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error engineering features: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "from crewai_tools import DirectoryReadTool\n",
    "\n",
    "docs_tool_a = DirectoryReadTool(directory='data.csv')\n",
    "# Create Feature Engineering agent\n",
    "feature_engineering_agent = Agent(\n",
    "    role=\"Feature Engineering Specialist\",\n",
    "    goal=\"Transform and optimize features for machine learning models and save the transformed file\",\n",
    "    backstory=\"Expert in feature scaling, encoding\",\n",
    "    tools=[docs_tool_a,\n",
    "        FeatureEngineeringTool(),\n",
    "        # FeatureEngineeringTool().generate_interaction_features\n",
    "    ],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create Feature Engineering task\n",
    "feature_engineering_task = Task(\n",
    "    description=\"\"\"\n",
    "    Load the file, convert categorical variables to numerical values and scale the numerical values to make the dataset model-ready.\n",
    "    \"\"\",\n",
    "    agent=feature_engineering_agent,\n",
    "    expected_output=\"transformed dataset saved. also provide short description about which technologies are used for feature engineering\"\n",
    ")\n",
    "\n",
    "# Create and run the crew\n",
    "crew = Crew(\n",
    "    agents=[feature_engineering_agent],\n",
    "    tasks=[feature_engineering_task],\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "result = crew.kickoff()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = Crew(\n",
    "    agents=[data_preprocessing_agent,feature_engineering_agent],\n",
    "    tasks=[data_preprocessing_task,feature_engineering_task],\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "result = crew.kickoff()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must precede any llm module imports\n",
    "\n",
    "from langtrace_python_sdk import langtrace\n",
    "\n",
    "langtrace.init(api_key = 'a8f171dce8f1c150104082f5adbe1a67710f292d255ba94c5082347f372e04af')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import tool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "# llm = LLM(\n",
    "#     model=\"ollama/mistral\",\n",
    "#     base_url=\"http://localhost:11434\"\n",
    "# )\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"groq/llama-3.3-70b-versatile\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "class CsvRAGtool(BaseTool):\n",
    "    name: str = \"CSV Query Tool\"\n",
    "    description: str = \"A tool that analyzes CSV data and answers questions about its content using natural language queries.\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        try:\n",
    "            llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "            # Add allow_dangerous_code=True to acknowledge the security implications\n",
    "            agent = create_csv_agent(\n",
    "                llm,\n",
    "                \"data.csv\",\n",
    "                verbose=True,\n",
    "                agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                handle_parsing_errors=True,\n",
    "                allow_dangerous_code=True\n",
    "            )\n",
    "\n",
    "            return agent.run(query)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing CSV query: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "from crewai_tools import DirectoryReadTool\n",
    "\n",
    "docs_tool_a = DirectoryReadTool(directory='data.csv')\n",
    "csv_rag = CsvRAGtool()\n",
    "feature_engineering_agent = Agent(\n",
    "    role=\"Feature Engineering Specialist\",\n",
    "    goal=\"Your task is to analyze the different features of data and tell that is feature engineering really required for data or not\",\n",
    "    backstory=\"Expert in feature scaling, encoding\",\n",
    "    tools=[docs_tool_a,csv_rag],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create Feature Engineering task\n",
    "feature_engineering_task = Task(\n",
    "    description=\"\"\"\n",
    "    1. Load the preprocessed dataset\n",
    "    2. Check if feature scaling really reqd or not\n",
    "    3. answer should be YES or NO along with a description\n",
    "    \"\"\",\n",
    "    agent=feature_engineering_agent,\n",
    "    expected_output=\"Final answer \",\n",
    "    human_input= True\n",
    ")\n",
    "\n",
    "# Create and run the crew\n",
    "crew = Crew(\n",
    "    agents=[feature_engineering_agent],\n",
    "    tasks=[feature_engineering_task],\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "result = crew.kickoff()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if data cleaning and feature engineering really required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'InstanceOf' from 'pydantic' (C:\\Users\\mayur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent, Task, Crew, Process, LLM\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tool\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\crewai\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrew\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Crew\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flow\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\crewai\\agent.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Literal, Optional, Union\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, InstanceOf, PrivateAttr, model_validator\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CacheHandler\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_builder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_agent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseAgent\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'InstanceOf' from 'pydantic' (C:\\Users\\mayur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import tool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "# llm = LLM(\n",
    "#     model=\"ollama/mistral\",\n",
    "#     base_url=\"http://localhost:11434\"\n",
    "# )\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"groq/llama-3.3-70b-versatile\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "class CsvRAGtool(BaseTool):\n",
    "    name: str = \"CSV Query Tool\"\n",
    "    description: str = \"A tool that analyzes CSV data and answers questions about its content using natural language queries.\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        try:\n",
    "            llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "            # Add allow_dangerous_code=True to acknowledge the security implications\n",
    "            agent = create_csv_agent(\n",
    "                llm,\n",
    "                \"data.csv\",\n",
    "                verbose=True,\n",
    "                agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                handle_parsing_errors=True,\n",
    "                allow_dangerous_code=True\n",
    "            )\n",
    "\n",
    "            return agent.run(query)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing CSV query: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "from crewai_tools import DirectoryReadTool\n",
    "\n",
    "docs_tool_a = DirectoryReadTool(directory='data.csv')\n",
    "csv_rag = CsvRAGtool()\n",
    "data_analysis_agent = Agent(\n",
    "    role=\"Data analysis Specialist\",\n",
    "    goal=\"Your task is to analyze the different features of data and tell that is data cleaning and feature engineering really required for data or not\",\n",
    "    backstory=\"Expert in data cleaning, feature scaling, encoding\",\n",
    "    tools=[docs_tool_a,csv_rag],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create Feature Engineering task\n",
    "data_analysis_task = Task(\n",
    "    description=\"\"\"\n",
    "    1. Load the dataset\n",
    "    2. Check data cleaning is required or not :\n",
    "      - Check if there are any null values in the data \n",
    "      - check if there are dduplicate values in the data \n",
    "    3. Feature engineering is required or not by analysing following points:\n",
    "      - Check each columns data types\n",
    "      - check min max values of columns for feature scaling \n",
    "    \n",
    "    3. answer should be YES or NO along with a description\n",
    "    \"\"\",\n",
    "    agent=feature_engineering_agent,\n",
    "    expected_output=\"\"\"Final answer should be in following format:\n",
    "    data cleaning : required or not \n",
    "    Feature engineering : required or not\n",
    "    \"\"\",\n",
    "\n",
    ")\n",
    "\n",
    "# Create and run the crew\n",
    "crew = Crew(\n",
    "    agents=[data_analysis_agent],\n",
    "    tasks=[data_analysis_task],\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "result = crew.kickoff()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import tool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "import pandas as pd\n",
    "from crewai_tools import BaseTool\n",
    "\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "# llm = LLM(\n",
    "#     model=\"ollama/mistral\",\n",
    "#     base_url=\"http://localhost:11434\"\n",
    "# )\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"groq/llama3-8b-8192\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "class CsvRAGtool(BaseTool):\n",
    "    name: str = \"CSV Query Tool\"\n",
    "    description: str = \"A tool that analyzes CSV data and answers questions about its content using natural language queries.\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        try:\n",
    "            llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "            # Add allow_dangerous_code=True to acknowledge the security implications\n",
    "            agent = create_csv_agent(\n",
    "                llm,\n",
    "                \"data.csv\",\n",
    "                verbose=True,\n",
    "                agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                handle_parsing_errors=True,\n",
    "                allow_dangerous_code=True\n",
    "            )\n",
    "\n",
    "            return agent.run(query)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing CSV query: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "from crewai_tools import DirectoryReadTool\n",
    "\n",
    "docs_tool_a = DirectoryReadTool(directory='data.csv')\n",
    "csv_rag = CsvRAGtool()\n",
    "feature_engineering_agent = Agent(\n",
    "    role=\"Feature selection Specialist\",\n",
    "    goal=\"Your task is to analyze the different features of data and tell that which features we can drop that are not usefull for the data to train model\",\n",
    "    backstory=\"Expert in feature selection\",\n",
    "    tools=[docs_tool_a,csv_rag],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create Feature Engineering task\n",
    "feature_engineering_task = Task(\n",
    "    description=\"\"\"\n",
    "    1. Load the preprocessed dataset\n",
    "    2. Check features that would not be having any impact on the dataset to train model\n",
    "    3. answer should be list of features to drop along with a description\n",
    "    \"\"\",\n",
    "    agent=feature_engineering_agent,\n",
    "    expected_output=\"Final answer \"\n",
    ")\n",
    "\n",
    "# Create and run the crew\n",
    "crew = Crew(\n",
    "    agents=[feature_engineering_agent],\n",
    "    tasks=[feature_engineering_task],\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "result = crew.kickoff()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "# Add allow_dangerous_code=True to acknowledge the security implications\n",
    "agent = create_csv_agent(\n",
    "    llm,\n",
    "    \"data.csv\",\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    allow_dangerous_code=True  # Add this line\n",
    ")\n",
    "\n",
    "agent.run(\"how many columns are there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsvRAGtool(BaseTool):\n",
    "    name: str = \"CSV Query Tool\"\n",
    "    description: str = \"A tool that analyzes CSV data and answers questions about its content using natural language queries.\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        try:\n",
    "            llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "            # Add allow_dangerous_code=True to acknowledge the security implications\n",
    "            agent = create_csv_agent(\n",
    "                llm,\n",
    "                \"data.csv\",\n",
    "                verbose=True,\n",
    "                agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                handle_parsing_errors=True,\n",
    "                allow_dangerous_code=True\n",
    "            )\n",
    "\n",
    "            return agent.run(query)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing CSV query: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to end ml model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Health_insurance.csv\")\n",
    "new_df = df.drop(columns=\"charges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess data by feature scaling and label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def create_artifacts_directory():\n",
    "    \"\"\"Create artifacts directory if it doesn't exist\"\"\"\n",
    "    if not os.path.exists('artifacts'):\n",
    "        os.makedirs('artifacts')\n",
    "\n",
    "def feature_scaler_encoder(new_df):\n",
    "    \"\"\"Prepare training data and save preprocessing objects\"\"\"\n",
    "    # Store the original column order\n",
    "    original_columns = new_df.columns.tolist()\n",
    "    \n",
    "    # Initialize preprocessing objects\n",
    "    label_encoders = {}\n",
    "    categorical_cols = new_df.select_dtypes(include=['object']).columns\n",
    "    numerical_cols = new_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Apply LabelEncoder to each categorical column\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        new_df[col] = le.fit_transform(new_df[col])\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    new_df[numerical_cols] = scaler.fit_transform(new_df[numerical_cols])\n",
    "    \n",
    "    # Ensure columns are in original order\n",
    "    new_df = new_df[original_columns]\n",
    "    \n",
    "    # Save preprocessing objects\n",
    "    create_artifacts_directory()\n",
    "    \n",
    "    # Save label encoders\n",
    "    with open('artifacts/label_encoders.pkl', 'wb') as f:\n",
    "        pickle.dump(label_encoders, f)\n",
    "    \n",
    "    # Save scaler\n",
    "    with open('artifacts/scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    # Save column order\n",
    "    with open('artifacts/columns.pkl', 'wb') as f:\n",
    "        pickle.dump(original_columns, f)\n",
    "    \n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training and saving it in pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = feature_scaler_encoder(new_df)\n",
    "# Splitting the dataset\n",
    "X = new_df\n",
    "y = df['charges']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join('artifacts', 'health_insurance.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Evaluate performance using different metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'R²: {r2}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Load saved pkl files, model and define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessing_objects():\n",
    "    \"\"\"Load saved preprocessing objects\"\"\"\n",
    "    try:\n",
    "        with open('artifacts/label_encoders.pkl', 'rb') as f:\n",
    "            label_encoders = pickle.load(f)\n",
    "        \n",
    "        with open('artifacts/scaler.pkl', 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        \n",
    "        # with open('artifacts/columns.pkl', 'rb') as f:\n",
    "        #     original_columns = pickle.load(f)\n",
    "\n",
    "        with open('artifacts/health_insurance.pkl', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        return label_encoders, scaler, model\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        raise Exception(\"Preprocessing objects not found. Please run training first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders, scaler, model = load_preprocessing_objects()\n",
    "\n",
    "def make_prediction(data):\n",
    "    \"\"\"Prepare new data for prediction using saved preprocessing objects\"\"\"\n",
    "    \n",
    "    # Convert input dictionary to DataFrame\n",
    "    input_df = pd.DataFrame([data])\n",
    "\n",
    "    # Encode categorical variables\n",
    "    categorical_cols = input_df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if col in label_encoders:\n",
    "            input_df[col] = label_encoders[col].transform(input_df[col].astype(str))\n",
    "    \n",
    "    # Scale numerical features\n",
    "    numerical_cols = input_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    input_df[numerical_cols] = scaler.transform(input_df[numerical_cols])\n",
    "\n",
    "    res = model.predict(input_df)\n",
    "    \n",
    "    return res[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Make Predictions for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example new data for prediction\n",
    "new_data = {\n",
    "    'age': 19,\n",
    "    'sex': 'female',\n",
    "    'bmi': 27,\n",
    "    'children': 0,\n",
    "    'smoker': 'yes',\n",
    "    'region': 'southwest'\n",
    "}\n",
    "\n",
    "res = make_prediction(new_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Trainer AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"artifacts/after_drop_col.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'InstanceOf' from 'pydantic' (C:\\Users\\mayur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent, Task, Crew, Process, LLM\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CodeInterpreterTool, DirectoryReadTool\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextwrap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dedent\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\crewai\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrew\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Crew\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flow\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\crewai\\agent.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Literal, Optional, Union\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, InstanceOf, PrivateAttr, model_validator\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CacheHandler\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_builder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_agent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseAgent\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'InstanceOf' from 'pydantic' (C:\\Users\\mayur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai_tools import CodeInterpreterTool, DirectoryReadTool\n",
    "from textwrap import dedent\n",
    "import os\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"groq/gemma2-9b-it\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class output_format(BaseModel):\n",
    "    model_code: str\n",
    "\n",
    "data_path = os.path.join('artifacts','engineered_features.csv')\n",
    "docs_tool = DirectoryReadTool(directory=data_path)\n",
    "\n",
    "\n",
    "# Create our ML Coding Agent\n",
    "ml_coder_agent = Agent(\n",
    "    role='Machine Learning Engineer',\n",
    "    goal='Write and optimize machine learning code to solve data science problems',\n",
    "    backstory=dedent(\"\"\"\n",
    "        You are an expert machine learning engineer with extensive experience in \n",
    "        implementing ML algorithms, data preprocessing, and model optimization. \n",
    "        You excel at writing clean, efficient Python code for ML tasks.\n",
    "    \"\"\"),\n",
    "    tools=[docs_tool , CodeInterpreterTool()],\n",
    "    verbose=True,\n",
    "    llm =llm\n",
    ")\n",
    "\n",
    "\n",
    "# Create Feature Engineering task\n",
    "ml_coder_task = Task(\n",
    "    description=\"\"\"\n",
    "    1. Load the preprocessed dataset\n",
    "    2. Assume target column as {target} and model type {model_type}\n",
    "    3. Do train test split\n",
    "    4. provide ML code for {model_type} model\n",
    "    \"\"\",\n",
    "    agent=ml_coder_agent,\n",
    "    expected_output=\"ML model code\",\n",
    "    output_pydantic=output_format\n",
    ")\n",
    "\n",
    "\n",
    "# Create and run the crew\n",
    "crew = Crew(\n",
    "    agents=[ml_coder_agent],\n",
    "    tasks=[ml_coder_task],\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "target = \"Price\"\n",
    "model_type = \"regression\"\n",
    "\n",
    "result = crew.kickoff(inputs={'target': target, 'model_type':model_type})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "\n",
      "# Load the preprocessed dataset\n",
      "features = pd.read_csv('/path/to/your/data/engineered_features.csv')\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "from sklearn.model_selection import train_test_split\n",
      "X = features.drop(columns=['Price'], axis=1)\n",
      "Y = features['Price']\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Choose a regression model (example: Linear Regression)\n",
      "from sklearn.linear_model import LinearRegression\n",
      "model = LinearRegression()\n",
      "\n",
      "# Train the model\n",
      "model.fit(X_train, y_train)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code = result[\"model_code\"]\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('engineered_features.csv')\n",
    "\n",
    "# Assume 'Price' is the target column\n",
    "y = df['Price']\n",
    "x = df.drop('Price', axis=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Random Forest regressor\n",
    "model = RandomForestRegressor()\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = model.predict(test_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'InstanceOf' from 'pydantic' (C:\\Users\\mayur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary tools\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CodeInterpreterTool, DirectoryReadTool\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent, Task, Crew, Process, LLM\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CodeInterpreterTool, DirectoryReadTool\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\crewai_tools\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     BraveSearchTool,\n\u001b[0;32m      3\u001b[0m     BrowserbaseLoadTool,\n\u001b[0;32m      4\u001b[0m     CodeDocsSearchTool,\n\u001b[0;32m      5\u001b[0m     CodeInterpreterTool,\n\u001b[0;32m      6\u001b[0m     ComposioTool,\n\u001b[0;32m      7\u001b[0m     CSVSearchTool,\n\u001b[0;32m      8\u001b[0m     DallETool,\n\u001b[0;32m      9\u001b[0m     DirectoryReadTool,\n\u001b[0;32m     10\u001b[0m     DirectorySearchTool,\n\u001b[0;32m     11\u001b[0m     DOCXSearchTool,\n\u001b[0;32m     12\u001b[0m     EXASearchTool,\n\u001b[0;32m     13\u001b[0m     FileReadTool,\n\u001b[0;32m     14\u001b[0m     FileWriterTool,\n\u001b[0;32m     15\u001b[0m     FirecrawlCrawlWebsiteTool,\n\u001b[0;32m     16\u001b[0m     FirecrawlScrapeWebsiteTool,\n\u001b[0;32m     17\u001b[0m     FirecrawlSearchTool,\n\u001b[0;32m     18\u001b[0m     GithubSearchTool,\n\u001b[0;32m     19\u001b[0m     JSONSearchTool,\n\u001b[0;32m     20\u001b[0m     LlamaIndexTool,\n\u001b[0;32m     21\u001b[0m     MDXSearchTool,\n\u001b[0;32m     22\u001b[0m     MultiOnTool,\n\u001b[0;32m     23\u001b[0m     MySQLSearchTool,\n\u001b[0;32m     24\u001b[0m     NL2SQLTool,\n\u001b[0;32m     25\u001b[0m     PDFSearchTool,\n\u001b[0;32m     26\u001b[0m     PGSearchTool,\n\u001b[0;32m     27\u001b[0m     RagTool,\n\u001b[0;32m     28\u001b[0m     ScrapeElementFromWebsiteTool,\n\u001b[0;32m     29\u001b[0m     ScrapeWebsiteTool,\n\u001b[0;32m     30\u001b[0m     ScrapflyScrapeWebsiteTool,\n\u001b[0;32m     31\u001b[0m     SeleniumScrapingTool,\n\u001b[0;32m     32\u001b[0m     SerperDevTool,\n\u001b[0;32m     33\u001b[0m     SerplyJobSearchTool,\n\u001b[0;32m     34\u001b[0m     SerplyNewsSearchTool,\n\u001b[0;32m     35\u001b[0m     SerplyScholarSearchTool,\n\u001b[0;32m     36\u001b[0m     SerplyWebpageToMarkdownTool,\n\u001b[0;32m     37\u001b[0m     SerplyWebSearchTool,\n\u001b[0;32m     38\u001b[0m     SpiderTool,\n\u001b[0;32m     39\u001b[0m     TXTSearchTool,\n\u001b[0;32m     40\u001b[0m     VisionTool,\n\u001b[0;32m     41\u001b[0m     WebsiteSearchTool,\n\u001b[0;32m     42\u001b[0m     XMLSearchTool,\n\u001b[0;32m     43\u001b[0m     YoutubeChannelSearchTool,\n\u001b[0;32m     44\u001b[0m     YoutubeVideoSearchTool,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_tool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTool, Tool, tool\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\crewai_tools\\tools\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrave_search_tool\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrave_search_tool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BraveSearchTool\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrowserbase_load_tool\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrowserbase_load_tool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BrowserbaseLoadTool\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_docs_search_tool\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_docs_search_tool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CodeDocsSearchTool\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\crewai_tools\\tools\\brave_search_tool\\brave_search_tool.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai_tools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_tool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTool\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_save_results_to_file\u001b[39m(content: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Saves the search results to a file.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\crewai_tools\\tools\\base_tool.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel \u001b[38;5;28;01mas\u001b[39;00m PydanticBaseModel\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_tool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTool\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstructured_tool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrewStructuredTool\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTool\u001b[39;00m(BaseTool):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\crewai\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrew\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Crew\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flow\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\crewai\\agent.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Literal, Optional, Union\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, InstanceOf, PrivateAttr, model_validator\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CacheHandler\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_builder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_agent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseAgent\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'InstanceOf' from 'pydantic' (C:\\Users\\mayur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "# Import necessary tools\n",
    "from crewai_tools import CodeInterpreterTool, DirectoryReadTool\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai_tools import CodeInterpreterTool, DirectoryReadTool\n",
    "from textwrap import dedent\n",
    "import os\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"groq/gemma2-9b-it\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "data_path = os.path.join('artifacts','after_drop_col.csv')\n",
    "docs_tool = DirectoryReadTool(directory=data_path)\n",
    "\n",
    "\n",
    "# Define Agents\n",
    "ml_code_writer = Agent(\n",
    "    role='ML Code Writer',\n",
    "    goal='Write efficient and scalable machine learning model code',\n",
    "    backstory='An experienced ML engineer with expertise in writing robust ML code.',\n",
    "    tools=[CodeInterpreterTool(), docs_tool],\n",
    "    llm = llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "ml_code_runner = Agent(\n",
    "    role='ML Model Code Runner',\n",
    "    goal='Execute machine learning model code and handle runtime environments',\n",
    "    backstory='A proficient coder with a knack for managing and running complex ML models.',\n",
    "    tools=[CodeInterpreterTool(), docs_tool],\n",
    "    llm = llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "ml_model_evaluator = Agent(\n",
    "    role='ML Model Evaluator',\n",
    "    goal='Evaluate the performance of machine learning models',\n",
    "    backstory='A meticulous analyst specializing in model evaluation and performance metrics.',\n",
    "    tools=[CodeInterpreterTool(), docs_tool],\n",
    "    llm = llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "extra_suggestions_agent = Agent(\n",
    "    role='Model Improvement Advisor',\n",
    "    goal='Provide suggestions to improve model accuracy if it is low',\n",
    "    backstory='A seasoned ML researcher with deep knowledge of optimization techniques.',\n",
    "    llm = llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define Tasks\n",
    "write_ml_code = Task(\n",
    "    description='Write code for a machine learning model based on the provided specifications.',\n",
    "    expected_output='A well-documented script implementing the specified ML model.',\n",
    "    agent=ml_code_writer\n",
    ")\n",
    "\n",
    "run_ml_code = Task(\n",
    "    description='Run the ML model code and ensure it executes correctly in the given environment.',\n",
    "    expected_output='Logs and results from the model run, including any errors or issues encountered.',\n",
    "    agent=ml_code_runner\n",
    ")\n",
    "\n",
    "evaluate_ml_model = Task(\n",
    "    description='Evaluate the ML model’s performance using appropriate metrics and datasets.',\n",
    "    expected_output='A detailed report on the model’s performance, including metrics like accuracy, precision, recall, and F1-score.',\n",
    "    agent=ml_model_evaluator\n",
    ")\n",
    "\n",
    "suggest_improvements = Task(\n",
    "    description='Analyze the evaluation report and suggest improvements if the model accuracy is low.',\n",
    "    expected_output='A list of actionable suggestions to improve the model’s accuracy, such as parameter tuning, feature engineering, or algorithm changes.',\n",
    "    agent=extra_suggestions_agent\n",
    ")\n",
    "\n",
    "# Assemble a crew with planning enabled\n",
    "crew = Crew(\n",
    "    agents=[ml_code_writer, ml_code_runner, ml_model_evaluator, extra_suggestions_agent],\n",
    "    tasks=[write_ml_code, run_ml_code, evaluate_ml_model, suggest_improvements],\n",
    "    verbose=True,\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "result = crew.kickoff()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'InstanceOf' from 'pydantic' (C:\\Users\\mayur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Must precede any llm module imports\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent, Task, Crew, Process, LLM\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tool\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\crewai\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrew\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Crew\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flow\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\crewai\\agent.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Literal, Optional, Union\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, InstanceOf, PrivateAttr, model_validator\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CacheHandler\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrewai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_builder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_agent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseAgent\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'InstanceOf' from 'pydantic' (C:\\Users\\mayur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "# Must precede any llm module imports\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import tool\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
    "from langchain_groq import ChatGroq\n",
    "from crewai_tools import BaseTool, DirectoryReadTool\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLM(\n",
    "    model=\"groq/gemma2-9b-it\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# llm = LLM(\n",
    "#     model=\"ollama/llama3.2\",\n",
    "#     base_url=\"http://localhost:11434\"\n",
    "# )\n",
    "\n",
    "class DataPreprocessor(BaseTool):\n",
    "    name: str = \"Data Preprocessor\"\n",
    "    description: str = \"Preprocesses data by handling missing values, removing duplicates, and encoding categorical variables.\"\n",
    "\n",
    "    def _run(self, file_path: str) -> str:\n",
    "        try:\n",
    "            # Load the data\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Get initial info\n",
    "            initial_shape = df.shape\n",
    "            initial_missing = df.isnull().sum().sum()\n",
    "            \n",
    "            # Calculate the percentage of missing values\n",
    "            missing_percentage = (initial_missing / (df.size)) * 100\n",
    "            \n",
    "            # Handle missing values\n",
    "            if missing_percentage < 5:\n",
    "                df = df.dropna()\n",
    "            else:\n",
    "                # Use SimpleImputer for numerical columns\n",
    "                num_cols = df.select_dtypes(include=['number']).columns\n",
    "                if not num_cols.empty:\n",
    "                    num_imputer = SimpleImputer(strategy='mean')\n",
    "                    df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "                \n",
    "                # Use SimpleImputer for categorical columns\n",
    "                cat_cols = df.select_dtypes(include=['object']).columns\n",
    "                if not cat_cols.empty:\n",
    "                    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "                    df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "            \n",
    "            # Remove duplicate entries\n",
    "            df = df.drop_duplicates()\n",
    "            \n",
    "            # Identify categorical columns\n",
    "            categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "            \n",
    "            # Get final info\n",
    "            final_shape = df.shape\n",
    "            final_missing = df.isnull().sum().sum()\n",
    "            \n",
    "            # Save the processed data\n",
    "            processed_file_path = os.path.join('artifacts', 'processed_data.csv')\n",
    "            os.makedirs(os.path.dirname(processed_file_path), exist_ok=True)\n",
    "            df.to_csv(processed_file_path, index=False)\n",
    "            \n",
    "            # return f\"\"\"\n",
    "            # Data preprocessing completed:\n",
    "            # - Initial shape: {initial_shape}\n",
    "            # - Initial missing values: {initial_missing}\n",
    "            # - Final shape: {final_shape}\n",
    "            # - Final missing values: {final_missing}\n",
    "            # - Categorical variables found: {categorical_columns}\n",
    "            # - Duplicates removed\n",
    "            # \"\"\"\n",
    "            return f\"Cleaned data saved to {processed_file_path}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error in preprocessing: {str(e)}\"\n",
    "\n",
    "class FeatureEngineeringTool(BaseTool):\n",
    "    name: str = \"Feature Scaling Tool\"\n",
    "    description: str = \"Scales numerical features and encodes categorical values\"\n",
    "\n",
    "    def _run(self, file_path: str, target: str, model:str) -> str:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df_engineered = df.copy()\n",
    "            \n",
    "            # Encode categorical variables\n",
    "            label_encoders = {}\n",
    "            categorical_cols = df_engineered.select_dtypes(include=['object']).columns\n",
    "            categorical_cols = [col for col in categorical_cols if col != target]  # Filter out the target column\n",
    "            for col in categorical_cols:\n",
    "                le = LabelEncoder()\n",
    "                df_engineered[col] = le.fit_transform(df_engineered[col].astype(str))\n",
    "                label_encoders[col] = le\n",
    "\n",
    "            # Create artifacts directory if it doesn't exist\n",
    "            os.makedirs('artifacts', exist_ok=True)\n",
    "            \n",
    "            # Save the label encoder\n",
    "            encoder_filename = os.path.join('artifacts', 'label_encoder.pkl')\n",
    "            with open(encoder_filename, 'wb') as file:\n",
    "                pickle.dump(label_encoders, file)\n",
    "\n",
    "            ## Check whether label encoding is necessory or not is model is classification\n",
    "            dtype_target = df_engineered[target].dtype\n",
    "            print(dtype_target)\n",
    "            if dtype_target == \"object\" and model == \"classification\":\n",
    "                print(\"Label encoding necessory\")\n",
    "                le_target = LabelEncoder()\n",
    "                df_engineered[target] = le_target.fit_transform(df_engineered[target].astype(str))\n",
    "                target_encoder_filename = os.path.join('artifacts', 'target_label_encoder.pkl')\n",
    "                with open(target_encoder_filename, 'wb') as file:\n",
    "                    pickle.dump(le_target, file)\n",
    "            else:\n",
    "                print(\"Not necessory\")\n",
    "            \n",
    "            # Scale numerical features\n",
    "            numerical_cols = df_engineered.select_dtypes(include=['int64', 'float64']).columns\n",
    "            numerical_cols = [col for col in numerical_cols if col != target]\n",
    "            if len(numerical_cols) > 0:\n",
    "                scaler = StandardScaler()\n",
    "                df_engineered[numerical_cols] = scaler.fit_transform(df_engineered[numerical_cols])\n",
    "                \n",
    "                # Save the scaler\n",
    "                scaler_filename = os.path.join('artifacts', 'scaler.pkl')\n",
    "                with open(scaler_filename, 'wb') as file:\n",
    "                    pickle.dump(scaler, file)\n",
    "\n",
    "            output_path = os.path.join('artifacts', 'engineered_features.csv')\n",
    "            df_engineered.to_csv(output_path, index=False)\n",
    "            \n",
    "            return f\"Feature engineering completed. File saved to {output_path}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error in feature engineering: {str(e)}\"\n",
    "\n",
    "class CsvRAGtool(BaseTool):\n",
    "    name: str = \"CSV Query Tool\"\n",
    "    description: str = \"Analyzes CSV data and answers questions using natural language queries.\"\n",
    "\n",
    "    def _run(self, query: str, file_path: str) -> str:\n",
    "        try:\n",
    "            llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
    "            agent = create_csv_agent(\n",
    "                llm,\n",
    "                file_path,\n",
    "                verbose=True,\n",
    "                agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                handle_parsing_errors=True,\n",
    "                allow_dangerous_code=True\n",
    "            )\n",
    "            return agent.run(query)\n",
    "        except Exception as e:\n",
    "            return f\"Error in CSV query: {str(e)}\"\n",
    "\n",
    "# Initialize paths and tools\n",
    "input_path = os.path.join('artifacts', 'after_drop_col.csv')\n",
    "processed_file_path = os.path.join('artifacts', 'processed_data.csv')\n",
    "\n",
    "# Initialize tools\n",
    "docs_tool_preprocessing = DirectoryReadTool(directory=input_path)\n",
    "data_processing_tool = DataPreprocessor()\n",
    "docs_tool_engineering = DirectoryReadTool(directory=processed_file_path)\n",
    "csv_rag = CsvRAGtool()\n",
    "feature_eng = FeatureEngineeringTool()\n",
    "\n",
    "# Create agents\n",
    "data_preprocessing_agent = Agent(\n",
    "    role=\"Data Preprocessing Specialist\",\n",
    "    goal=\"Load, clean, and perform initial transformations on datasets\",\n",
    "    backstory=\"Expert in data cleaning and preprocessing using pandas, numpy, and sklearn libraries\",\n",
    "    llm=llm,\n",
    "    tools=[docs_tool_preprocessing, data_processing_tool],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "feature_engineering_agent = Agent(\n",
    "    role=\"Feature Engineering Specialist\",\n",
    "    goal=\"Analyze features and perform feature engineering if required\",\n",
    "    backstory=\"Expert in feature scaling and encoding\",\n",
    "    tools=[docs_tool_engineering, csv_rag, feature_eng],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create tasks\n",
    "data_preprocessing_task = Task(\n",
    "    description=\"\"\"\n",
    "    1. Load the file 'artifacts/after_drop_col.csv'\n",
    "    2. Handle missing values (remove if <5% missing, else use imputer)\n",
    "    3. Remove duplicates\n",
    "    Save the processed dataset.\n",
    "    \"\"\",\n",
    "    expected_output='Processed dataset saved successfully',\n",
    "    agent=data_preprocessing_agent\n",
    ")\n",
    "\n",
    "feature_engineering_task = Task(\n",
    "    description=\"\"\"\n",
    "    1. Load the cleaned data\n",
    "    2. Analyze if feature engineering is required\n",
    "    3. input variable from user target - {target}, model - {model}\n",
    "    4. If required, perform feature scaling and encoding\n",
    "    5. Provide justification for decisions made and it should be in short\n",
    "    \"\"\",\n",
    "    agent=feature_engineering_agent,\n",
    "    expected_output=\"Analysis and feature engineering report\"\n",
    ")\n",
    "\n",
    "# Create and run the crew\n",
    "crew = Crew(\n",
    "    agents=[data_preprocessing_agent, feature_engineering_agent],\n",
    "    tasks=[data_preprocessing_task, feature_engineering_task],\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "target = 'Price'\n",
    "model = 'regression'\n",
    "# Execute the pipeline\n",
    "try:\n",
    "    result = crew.kickoff(inputs={'target': target, 'model': model})\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error executing pipeline: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import pickle\n",
    "df = pd.read_csv(\"artifacts/after_drop_col.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_engineered = df.copy()\n",
    "\n",
    "target = \"Garage\"\n",
    "model = \"classification\"\n",
    "\n",
    "## Check whether label encoding is necessory or not is model is classification\n",
    "dtype_target = df_engineered[target].dtype\n",
    "if dtype_target == \"object\" and model == \"classification\":\n",
    "    print(\"Label encoding necessory\")\n",
    "    le_target = LabelEncoder()\n",
    "    df_engineered[target] = le_target.fit_transform(df_engineered[target].astype(str))\n",
    "    target_encoder_filename = os.path.join('artifacts', 'target_label_encoder.pkl')\n",
    "    with open(target_encoder_filename, 'wb') as file:\n",
    "        pickle.dump(le_target, file)\n",
    "else:\n",
    "    print(\"Not necessory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class feature_engineering_tool(BaseModel):\n",
    "    name: str = \"Feature Scaling Tool\"\n",
    "    description: str = \"Scales numerical features and encodes categorical values\"\n",
    "\n",
    "    def _run(self, file_path: str, target: str, model: str) -> str:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df_engineered = df.copy()\n",
    "\n",
    "            # Encode categorical variables\n",
    "            label_encoders = {}\n",
    "            categorical_cols = df_engineered.select_dtypes(include=['object']).columns\n",
    "            categorical_cols = [col for col in categorical_cols if col != target]  # Filter out the target column\n",
    "            for col in categorical_cols:\n",
    "                le = LabelEncoder()\n",
    "                df_engineered[col] = le.fit_transform(df_engineered[col].astype(str))\n",
    "                label_encoders[col] = le\n",
    "\n",
    "            # Create artifacts directory if it doesn't exist\n",
    "            os.makedirs('artifacts', exist_ok=True)\n",
    "\n",
    "            # Save the label encoder\n",
    "            encoder_filename = os.path.join('artifacts', 'label_encoder.pkl')\n",
    "            with open(encoder_filename, 'wb') as file:\n",
    "                pickle.dump(label_encoders, file)\n",
    "\n",
    "            ## Check whether label encoding is necessory or not is model is classification\n",
    "            dtype_target = df_engineered[target].dtype\n",
    "            print(dtype_target)\n",
    "            if dtype_target == \"object\" and model == \"classification\":\n",
    "                print(\"Label encoding necessory\")\n",
    "                le_target = LabelEncoder()\n",
    "                df_engineered[target] = le_target.fit_transform(df_engineered[target].astype(str))\n",
    "                target_encoder_filename = os.path.join('artifacts', 'target_label_encoder.pkl')\n",
    "                with open(target_encoder_filename, 'wb') as file:\n",
    "                    pickle.dump(le_target, file)\n",
    "            else:\n",
    "                print(\"Not necessory\")\n",
    "\n",
    "            # Scale numerical features\n",
    "            numerical_cols = df_engineered.select_dtypes(include=['int64', 'float64']).columns\n",
    "            if numerical_cols.size > 0:\n",
    "                scaler = StandardScaler()\n",
    "                df_engineered[numerical_cols] = scaler.fit_transform(df_engineered[numerical_cols])\n",
    "\n",
    "                # Save the scaler\n",
    "                scaler_filename = os.path.join('artifacts', 'scaler.pkl')\n",
    "                with open(scaler_filename, 'wb') as file:\n",
    "                    pickle.dump(scaler, file)\n",
    "\n",
    "            output_path = os.path.join('artifacts', 'engineered_features.csv')\n",
    "            df_engineered.to_csv(output_path, index=False)\n",
    "\n",
    "            return f\"Feature engineering completed. File saved to {output_path}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error in feature engineering: {str(e)}\"\n",
    "\n",
    "\n",
    "# Create an instance of the tool and run it\n",
    "tool_instance = feature_engineering_tool()\n",
    "result = tool_instance._run(\"artifacts/after_drop_col.csv\", \"Garage\", \"classification\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = LLM(\n",
    "    model=\"groq/gemma2-9b-it\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "class CsvRAGtool(BaseTool):\n",
    "    name: str = \"CSV Query Tool\"\n",
    "    description: str = \"Analyzes CSV data and provides insights through natural language queries.\"\n",
    "\n",
    "    def _run(self, query: str, file_path: str) -> str:\n",
    "        try:\n",
    "            llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
    "            agent = create_csv_agent(\n",
    "                llm,\n",
    "                file_path,\n",
    "                verbose=True,\n",
    "                agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                handle_parsing_errors=True,\n",
    "                allow_dangerous_code=True\n",
    "            )\n",
    "            return agent.run(query)\n",
    "        except Exception as e:\n",
    "            return f\"Error in CSV query: {str(e)}\"\n",
    "\n",
    "# Initialize paths and tools\n",
    "input_path = os.path.join('artifacts', 'after_drop_col.csv')\n",
    "\n",
    "# Initialize tools\n",
    "docs_tool_preprocessing = DirectoryReadTool(directory=input_path)\n",
    "csv_rag = CsvRAGtool()\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class PreprocessingSteps(BaseModel):\n",
    "    handle_missing_values: str\n",
    "    remove_duplicates: str\n",
    "    label_encode_categorical_columns: str\n",
    "    scale_numerical_columns: str\n",
    "\n",
    "# Create agents\n",
    "preprocessing_checker_agent = Agent(\n",
    "    role=\"Preprocessing Checker\",\n",
    "    goal=\"Assess and recommend necessary preprocessing steps for the dataset\",\n",
    "    backstory=\"An expert in data cleaning and preprocessing using pandas, numpy, and sklearn libraries\",\n",
    "    llm=llm,\n",
    "    tools=[csv_rag, docs_tool_preprocessing],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create tasks\n",
    "preprocessing_checker_task = Task(\n",
    "    description=\"\"\"\n",
    "    1. Load the dataset 'artifacts/after_drop_col.csv'.\n",
    "    2. Evaluate and determine if the following preprocessing steps are required:\n",
    "       - Handling missing values: required or not (check for null values).\n",
    "       - Removing duplicates: required or not\n",
    "       - Label encoding categorical columns: required or not\n",
    "       - Scaling numerical columns: required or not\n",
    "    3. Save the results as a JSON file indicating whether each step is required or not.\n",
    "    \"\"\",\n",
    "    expected_output='A JSON file indicating the necessity of each preprocessing step',\n",
    "    agent=preprocessing_checker_agent,\n",
    "    output_pydantic=PreprocessingSteps,\n",
    ")\n",
    "\n",
    "# Create and run the crew\n",
    "crew = Crew(\n",
    "    agents=[preprocessing_checker_agent],\n",
    "    tasks=[preprocessing_checker_task],\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = crew.kickoff()\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error executing pipeline: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
